{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Search words - separate them by comma: asad ,qweqrq\n",
      "Enter tweets to be pulled for each search word: how are you\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'how are you'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f91a81a5f428>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f91a81a5f428>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0msearch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter Search words - separate them by comma: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mTotal_tweet_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter tweets to be pulled for each search word: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;31m#print search_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0msearch_words_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'how are you'"
     ]
    }
   ],
   "source": [
    "# Application to pull streaming data from twitter and determine the sentiment of them.\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import sys\n",
    "import webbrowser\n",
    "import codecs\n",
    "import csv\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class tweetlistener(StreamListener):\n",
    "\n",
    "\n",
    "    def on_status(self,status):\n",
    "        global counter,Total_tweet_count,outfile,search_words_list,indiv,outfile\n",
    "        counter += 1\n",
    "        if counter >= Total_tweet_count:\n",
    "            search_words_list.pop(0)\n",
    "            outfile.close()\n",
    "            senti1 = Sentiment()\n",
    "            senti1.sentiment_analysis()\n",
    "            #time.sleep(15)\n",
    "            search_tweets()\n",
    "\n",
    "        try:\n",
    "            print (\"----------NEW TWEET ARRIVED!-----------\")\n",
    "            print (\"Tweet Text : %s\") % status.text\n",
    "            outfile.write(status.text)\n",
    "            outfile.write(str(\"\\n\"))\n",
    "            print (\"Author's Screen name : %s\") % status.author.screen_name\n",
    "            print (\"Time of creation : %s\") % status.created_at\n",
    "            print (\"Source of Tweet : %s\") % status.source\n",
    "        except UnicodeEncodeError:\n",
    "            print (\"Skipping a tweet\")\n",
    "\n",
    "    def on_error(self, status):\n",
    "        drawing()\n",
    "        print (\"Too soon reconnected . Will terminate the program\")\n",
    "        print (status)\n",
    "        sys.exit()\n",
    "\n",
    "class Sentiment():\n",
    "    def sentiment_analysis(self):\n",
    "        global file2,indiv,outfile,labels,colors,all_figs\n",
    "        pos_sent = open(\"positive_words.txt\").read()\n",
    "        positive_words = pos_sent.split('\\n')\n",
    "        positive_counts = []\n",
    "        neg_sent = open('negative_words.txt').read()\n",
    "        negative_words = neg_sent.split('\\n')\n",
    "        outfile.close()\n",
    "        negative_counts = []\n",
    "        conclusion = []\n",
    "        tweets_list = []\n",
    "        tot_pos = 0\n",
    "        tot_neu = 0\n",
    "        tot_neg = 0\n",
    "        all_total = 0\n",
    "        #print file2\n",
    "        tweets = codecs.open(file2, 'r', \"utf-8\").read()\n",
    "        tweet_list_dup = []\n",
    "\n",
    "        tweets_list = tweets.split('\\n')\n",
    "        #print tweets_list\n",
    "\n",
    "        for tweet in tweets_list:\n",
    "            positive_counter = 0\n",
    "            negative_counter = 0\n",
    "            tweet = tweet.encode(\"utf-8\")\n",
    "            tweet_list_dup.append(tweet)\n",
    "            tweet_processed = tweet.lower()\n",
    "\n",
    "            for p in list(punctuation):\n",
    "                tweet_processed = tweet_processed.replace(p, '')\n",
    "\n",
    "            words = tweet_processed.split(' ')\n",
    "            word_count = len(words)\n",
    "            for word in words:\n",
    "                if word in positive_words:\n",
    "                    positive_counter = positive_counter + 1\n",
    "                elif word in negative_words:\n",
    "                    negative_counter = negative_counter + 1\n",
    "\n",
    "            positive_counts.append(positive_counter)\n",
    "            negative_counts.append(negative_counter)\n",
    "            if positive_counter > negative_counter:\n",
    "                conclusion.append(\"Positive\")\n",
    "                tot_pos += 1\n",
    "            elif positive_counter == negative_counter:\n",
    "                conclusion.append(\"Neutral\")\n",
    "                tot_neu += 0.5\n",
    "            else:\n",
    "                conclusion.append(\"Negative\")\n",
    "                tot_neg +=1\n",
    "\n",
    "        #print len(positive_counts)\n",
    "        output = zip(tweet_list_dup, positive_counts, negative_counts,conclusion)\n",
    "        #output = output.encode('utf-8')\n",
    "\n",
    "        print (\"******** Overall Analysis **************\")\n",
    "\n",
    "\n",
    "        if tot_pos > tot_neg and tot_pos > tot_neu:\n",
    "            print (\"Overall Sentiment - Positive\")\n",
    "        elif tot_neg > tot_pos and tot_neg > tot_neu:\n",
    "            print (\"Overall Sentiment - Negative\")\n",
    "        elif tot_neg == tot_neu and tot_neg > tot_pos:\n",
    "            print (\"Overall Sentiment - Negative\")\n",
    "        elif tot_pos + tot_neg < tot_neu:\n",
    "            print (\"Overall Sentiment - Semi Positive \")\n",
    "        else:\n",
    "            print (\"Overall Sentiment - Neutral\")\n",
    "\n",
    "\n",
    "        print (\"%%%%%%%%%%%% End of stream - \" + indiv + \"   %%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "        file1 = 'tweet_sentiment_' + indiv + '.csv'\n",
    "        writer = csv.writer(open(file1, 'wb'))\n",
    "        writer.writerows(output)\n",
    "        draw_helper = []\n",
    "        draw_helper.append(tot_pos)\n",
    "        draw_helper.append(tot_neg)\n",
    "        draw_helper.append(tot_neu)\n",
    "        draw_helper.append(indiv)\n",
    "        all_figs.append(draw_helper)\n",
    "\n",
    "        #figs.append(drawing())\n",
    "\n",
    "\n",
    "def drawing():\n",
    "        global all_figs\n",
    "        for one_fig in all_figs:\n",
    "            all_total = 0\n",
    "            sentiments = {}\n",
    "            sentiments[\"Positive\"] = one_fig[0]\n",
    "            sentiments[\"Negative\"] = one_fig[1]\n",
    "            sentiments[\"Neutral\"]  = one_fig[2]\n",
    "            all_total = one_fig[0] + one_fig[1] + one_fig[2]\n",
    "            sizes = []\n",
    "\n",
    "            sizes = [sentiments['Positive']/float(all_total), sentiments['Negative']/float(all_total), sentiments['Neutral']/float(all_total)]\n",
    "\n",
    "\n",
    "            plt.pie(sizes,labels=labels, colors=colors, autopct='%1.1f%%', shadow=True)\n",
    "            plt.axis('equal')\n",
    "\n",
    "            plt.title('sentiment for the word - ' + str(one_fig[3]))\n",
    "            fig_name = \"fig_\" + str(one_fig[3]) + \".png\"\n",
    "            # Save the figures\n",
    "            plt.savefig(fig_name)\n",
    "            plt.close()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    global Total_tweet_count,outfile,file,search_words_list,auth,labels,colors,all_figs\n",
    "    consumer_key = 'L4ROQj8rhzrJ269XOfdUlSh43'\n",
    "    consumer_secret = 'WWReiZIgd8QwgPdiHZHMb84r4rulcVOw4uhUAOyotTC36DlAXu '\n",
    "    access_token = '1163443965988724737-ivY7BvxXUiGf9ShpRCuIEvh3JIJsWD'\n",
    "    access_secret = '46KVcnVm8zZ4XCASebwYNbVkDsrYVI1RRu9jm4Pg0I6dm'\n",
    "\n",
    "    search_words = str(input(\"Enter Search words - separate them by comma: \"))\n",
    "    Total_tweet_count = int(input(\"Enter tweets to be pulled for each search word: \"))\n",
    "    #print search_words\n",
    "    search_words_list = search_words.split(\",\")\n",
    "    Total_tweet_count = 10\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    labels = ['Positive','Negative','Neutral']\n",
    "    colors = ['yellowgreen','lightcoral','gold']\n",
    "    all_figs= []\n",
    "    search_tweets()\n",
    "    outfile = codecs.open(\"F:\\\\test_tweets1.txt\", 'w', \"utf-8\")#iphone\n",
    "\n",
    "def search_tweets():\n",
    "    global search_words_list,counter,auth,indiv,outfile,file2,plt,access\n",
    "    consumer_key = 'L4ROQj8rhzrJ269XOfdUlSh43'\n",
    "    consumer_secret = 'WWReiZIgd8QwgPdiHZHMb84r4rulcVOw4uhUAOyotTC36DlAXu '\n",
    "    access_token = '1163443965988724737-ivY7BvxXUiGf9ShpRCuIEvh3JIJsWD'\n",
    "    access_secret = '46KVcnVm8zZ4XCASebwYNbVkDsrYVI1RRu9jm4Pg0I6dm'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    #auth.set_access_token(access_token, access_secret)\n",
    "    print (search_words_list)\n",
    "    for indiv in search_words_list:\n",
    "        #indiv = indiv.split()\n",
    "        print (\"Search Word - \" + indiv + \" - is being processed\")\n",
    "        counter = 0\n",
    "        file2 = \"test_\" + str(indiv[0]) + \".txt\"\n",
    "        outfile = codecs.open(file2, 'w', \"utf-8\")\n",
    "        twitterStream = Stream(auth, tweetlistener())\n",
    "        one_list = []\n",
    "        one_list.append(indiv)\n",
    "        print (one_list)\n",
    "        twitterStream.filter(track=one_list,languages = [\"en\"])\n",
    "    #for i in range(len(figs)):\n",
    "    drawing()\n",
    "    sys.exit()\n",
    "    \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Search words - separate them by comma: pakistan,india\n",
      "Enter tweets to be pulled for each search word: 20\n",
      "['pakistan', 'india']\n",
      "Search Word - pakistan - is being processed\n",
      "['pakistan']\n",
      "Too soon reconnected . Will terminate the program\n",
      "401\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print((cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
